<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Ray on GKE using DraNet | DraNet</title>
<meta name=description content="To get started, follow the instructions to create a GKE cluster with DRA support and using DraNet, it is important to follow the instructions, since there are multiple dependencies on the Kubernetes API version, the RDMA NCCL installer and the DraNet component.
The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.
Deploy RayCluster Install Ray CRDs and the KubeRay operator:"><meta property="og:url" content="https://dranet.dev/docs/user/kuberay/"><meta property="og:site_name" content="DraNet"><meta property="og:title" content="Ray on GKE using DraNet"><meta property="og:description" content="To get started, follow the instructions to create a GKE cluster with DRA support and using DraNet, it is important to follow the instructions, since there are multiple dependencies on the Kubernetes API version, the RDMA NCCL installer and the DraNet component.
The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.
Deploy RayCluster Install Ray CRDs and the KubeRay operator:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-07-14T10:10:40+00:00"><meta property="article:modified_time" content="2025-07-14T10:10:40+00:00"><meta itemprop=name content="Ray on GKE using DraNet"><meta itemprop=description content="To get started, follow the instructions to create a GKE cluster with DRA support and using DraNet, it is important to follow the instructions, since there are multiple dependencies on the Kubernetes API version, the RDMA NCCL installer and the DraNet component.
The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.
Deploy RayCluster Install Ray CRDs and the KubeRay operator:"><meta itemprop=datePublished content="2025-07-14T10:10:40+00:00"><meta itemprop=dateModified content="2025-07-14T10:10:40+00:00"><meta itemprop=wordCount content="1340"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ray on GKE using DraNet"><meta name=twitter:description content="To get started, follow the instructions to create a GKE cluster with DRA support and using DraNet, it is important to follow the instructions, since there are multiple dependencies on the Kubernetes API version, the RDMA NCCL installer and the DraNet component.
The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.
Deploy RayCluster Install Ray CRDs and the KubeRay operator:"><link rel=preload href=/scss/main.min.3bb6570761fbbb25e6691001febc67f331f0953db4e4e1cfb79172c2e6a5819e.css as=style integrity="sha256-O7ZXB2H7uyXmaRAB/rxn8zHwlT205OHPt5FywualgZ4=" crossorigin=anonymous><link href=/scss/main.min.3bb6570761fbbb25e6691001febc67f331f0953db4e4e1cfb79172c2e6a5819e.css rel=stylesheet integrity="sha256-O7ZXB2H7uyXmaRAB/rxn8zHwlT205OHPt5FywualgZ4=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YH3W884R6Z"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YH3W884R6Z")}</script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>DraNet</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/docs/user><span>User Guides</span></a></li><li class=nav-item><a class=nav-link href=/docs/concepts><span>Concepts</span></a></li><li class=nav-item><a class=nav-link href=/docs/contributing><span>Contributing</span></a></li></ul></div><div class="d-none d-lg-block"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="td-sidebar-nav collapse" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docs-li><a href=/docs/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-docs><span>DraNet</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsquick-start-li><a href=/docs/quick-start/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsquick-start><span>Quick Start</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docsuser-li><a href=/docs/user/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsuser><span>User Guides</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-docsuserkuberay-li><a href=/docs/user/kuberay/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id=m-docsuserkuberay><span class=td-sidebar-nav-active-item>Ray on GKE using DraNet</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsusernvidia-dranet-li><a href=/docs/user/nvidia-dranet/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsusernvidia-dranet><span>GKE with NVIDIA DRA and DraNet</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsusergke-tpu-performance-li><a href=/docs/user/gke-tpu-performance/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsusergke-tpu-performance><span>GKE and Cloud TPU v6e (Trillium)</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsusergke-rdma-li><a href=/docs/user/gke-rdma/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsusergke-rdma><span>GKE and GPUDirect RDMA with DRA</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsusermpi-operator-li><a href=/docs/user/mpi-operator/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsusermpi-operator><span>MPI Operator on GKE and GPUDirect RDMA</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsuserinterface-configuration-li><a href=/docs/user/interface-configuration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsuserinterface-configuration><span>Interface Configuration</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsconcepts-li><a href=/docs/concepts/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsconcepts><span>Concepts</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptslinux-network-interfaces-li><a href=/docs/concepts/linux-network-interfaces/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptslinux-network-interfaces><span>Linux Network Namespaces and Interfaces</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsflexible-networks-li><a href=/docs/concepts/flexible-networks/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsflexible-networks><span>Making Networks Flexible</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsinterface-status-li><a href=/docs/concepts/interface-status/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsinterface-status><span>Interface Status</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptshardware-efficiency-li><a href=/docs/concepts/hardware-efficiency/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptshardware-efficiency><span>Hardware Efficiency</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsrdma-li><a href=/docs/concepts/rdma/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsrdma><span>RDMA</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsrdma-modes-li><a href=/docs/concepts/rdma-modes/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsrdma-modes><span>RDMA Device Handling</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptshowitworks-li><a href=/docs/concepts/howitworks/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptshowitworks><span>How It Works</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsreferences-li><a href=/docs/concepts/references/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsreferences><span>References</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docscontributing-li><a href=/docs/contributing/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docscontributing><span>Contributing</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docscontributingdeveloper-guide-li><a href=/docs/contributing/developer-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributingdeveloper-guide><span>Developer Guide</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docscontributingcontributing-li><a href=/docs/contributing/contributing/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributingcontributing><span>Contributing</span></a></li></ul></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"><a href=https://github.com/google/dranet/tree/main/site/content/docs/user/kuberay.md class="td-page-meta--view td-page-meta__view" target=_blank rel=noopener><i class="fa-solid fa-file-lines fa-fw"></i> View page source</a>
<a href=https://github.com/google/dranet/edit/main/site/content/docs/user/kuberay.md class="td-page-meta--edit td-page-meta__edit" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Edit this page</a>
<a href="https://github.com/google/dranet/new/main/site/content/docs/user?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class="td-page-meta--child td-page-meta__child" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Create child page</a>
<a href="https://github.com/google/dranet/issues/new?title=Ray%20on%20GKE%20using%20DraNet" class="td-page-meta--issue td-page-meta__issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create documentation issue</a></div><div class=td-toc><nav id=TableOfContents><ul><li><ul><li><a href=#deploy-raycluster>Deploy RayCluster</a></li></ul></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/docs/>DraNet</a></li><li class=breadcrumb-item><a href=/docs/user/>User Guides</a></li><li class="breadcrumb-item active" aria-current=page>Ray on GKE using DraNet</li></ol></nav><div class=td-content><h1>Ray on GKE using DraNet</h1><header class=article-meta></header><p>To get started, follow the instructions to create a <a href=/docs/user/gke-rdma>GKE cluster with DRA
support and using DraNet</a>, it is important to follow the
instructions, since there are multiple dependencies on the Kubernetes API
version, the RDMA NCCL installer and the DraNet component.</p><p>The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.</p><h3 id=deploy-raycluster>Deploy RayCluster</h3><p>Install Ray CRDs and the KubeRay operator:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create -k <span style=color:#a31515>&#34;github.com/ray-project/kuberay/ray-operator/config/default?ref=v1.4.1&#34;</span>
</span></span></code></pre></div><p>We create one <code>ResourceClaimTemplate</code>, for the RDMA devices on the node, along
with a <code>DeviceClass</code> for the RDMA device.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: resource.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: DeviceClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dranet
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selectors:
</span></span><span style=display:flex><span>    - cel:
</span></span><span style=display:flex><span>        expression: device.driver == &#34;dra.net&#34;
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resource.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ResourceClaimTemplate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: all-nic
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    devices:
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>      - name: nic
</span></span><span style=display:flex><span>        deviceClassName: dranet
</span></span><span style=display:flex><span>        count: 8
</span></span><span style=display:flex><span>        selectors:
</span></span><span style=display:flex><span>        - cel:
</span></span><span style=display:flex><span>            expression: device.attributes[&#34;dra.net&#34;].rdma == true
</span></span></code></pre></div><p>Until the official Ray images support NVIDIA B200 with CUDA capability sm_100
you need to build a custom image:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> rayproject/ray:2.47.1-py39-cu128</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>USER</span><span style=color:#a31515> root</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> python -m pip install --upgrade pip<span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> pip uninstall cupy-cuda12x -y &amp;&amp; conda install -c conda-forge cupy<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> pip install --no-cache-dir --force-reinstall numpy==1.26.4<span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> pip install --no-cache-dir --force-reinstall scipy==1.11.4<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>RUN</span> apt-get update &amp;&amp; apt-get -y install libnl-3-200 libnl-route-3-200<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#00f>USER</span><span style=color:#a31515> 1000</span><span>
</span></span></span></code></pre></div><p>Install a RayCluster and use the RDMA NICs on the workers nodes, you need to
specify some NCCL environment variables for optimal performance on Google Cloud
RDMA network:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: ray.io/v1
</span></span><span style=display:flex><span>kind: RayCluster
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: a4-ray-cluster
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  headGroupSpec:
</span></span><span style=display:flex><span>    rayStartParams:
</span></span><span style=display:flex><span>      dashboard-host: <span style=color:#a31515>&#39;0.0.0.0&#39;</span>
</span></span><span style=display:flex><span>    template:
</span></span><span style=display:flex><span>      spec:
</span></span><span style=display:flex><span>        containers:
</span></span><span style=display:flex><span>        - name: ray-head
</span></span><span style=display:flex><span>          image: aojea/ray:2.44.1-py39-cu128
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>          - containerPort: 6379
</span></span><span style=display:flex><span>            name: gcs-server
</span></span><span style=display:flex><span>          - containerPort: 8265
</span></span><span style=display:flex><span>            name: dashboard
</span></span><span style=display:flex><span>          - containerPort: 10001
</span></span><span style=display:flex><span>            name: client
</span></span><span style=display:flex><span>  workerGroupSpecs:
</span></span><span style=display:flex><span>  - replicas: 2
</span></span><span style=display:flex><span>    minReplicas: 0
</span></span><span style=display:flex><span>    maxReplicas: 4
</span></span><span style=display:flex><span>    groupName: gpu-group
</span></span><span style=display:flex><span>    rayStartParams: {}
</span></span><span style=display:flex><span>    template:
</span></span><span style=display:flex><span>      spec:
</span></span><span style=display:flex><span>        containers:
</span></span><span style=display:flex><span>        - name: ray-worker
</span></span><span style=display:flex><span>          image: aojea/ray:2.44.1-py39-cu128
</span></span><span style=display:flex><span>          resources:
</span></span><span style=display:flex><span>            limits:
</span></span><span style=display:flex><span>              cpu: <span style=color:#a31515>&#34;200&#34;</span>
</span></span><span style=display:flex><span>              memory: <span style=color:#a31515>&#34;1600Gi&#34;</span>
</span></span><span style=display:flex><span>              nvidia.com/gpu: <span style=color:#a31515>&#34;8&#34;</span>
</span></span><span style=display:flex><span>            requests:
</span></span><span style=display:flex><span>              cpu: <span style=color:#a31515>&#34;120&#34;</span>
</span></span><span style=display:flex><span>              memory: <span style=color:#a31515>&#34;1600Gi&#34;</span>
</span></span><span style=display:flex><span>              nvidia.com/gpu: <span style=color:#a31515>&#34;8&#34;</span>
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>          - name: LD_LIBRARY_PATH
</span></span><span style=display:flex><span>            value: /usr/local/nvidia/lib64
</span></span><span style=display:flex><span>          - name: TORCH_DISTRIBUTED_DEBUG
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;INFO&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_DEBUG
</span></span><span style=display:flex><span>            value: INFO <span style=color:green># Or &#34;WARN&#34;, &#34;DEBUG&#34;, &#34;TRACE&#34; for more verbosity</span>
</span></span><span style=display:flex><span>          - name: NCCL_DEBUG_SUBSYS
</span></span><span style=display:flex><span>            value: INIT,NET,ENV,COLL,GRAPH
</span></span><span style=display:flex><span>          - name: NCCL_NET
</span></span><span style=display:flex><span>            value: gIB
</span></span><span style=display:flex><span>          - name: NCCL_CROSS_NIC
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_NET_GDR_LEVEL
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;PIX&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_P2P_NET_CHUNKSIZE
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;131072&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_NVLS_CHUNKSIZE
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;524288&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_IB_ADAPTIVE_ROUTING
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_IB_QPS_PER_CONNECTION
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_IB_TC
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;52&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_IB_FIFO_TC
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;84&#34;</span>
</span></span><span style=display:flex><span>          - name: NCCL_TUNER_CONFIG_PATH
</span></span><span style=display:flex><span>            value: <span style=color:#a31515>&#34;/usr/local/gib/configs/tuner_config_a4.txtpb&#34;</span>
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>          - name: library-dir-host
</span></span><span style=display:flex><span>            mountPath: /usr/local/nvidia
</span></span><span style=display:flex><span>          - name: gib
</span></span><span style=display:flex><span>            mountPath: /usr/local/gib
</span></span><span style=display:flex><span>          - name: shared-memory
</span></span><span style=display:flex><span>            mountPath: /dev/shm
</span></span><span style=display:flex><span>        resourceClaims:
</span></span><span style=display:flex><span>          - name: nics
</span></span><span style=display:flex><span>            resourceClaimTemplateName: all-nic
</span></span><span style=display:flex><span>        tolerations:
</span></span><span style=display:flex><span>          - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>            operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>            effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        volumes:
</span></span><span style=display:flex><span>          - name: library-dir-host
</span></span><span style=display:flex><span>            hostPath:
</span></span><span style=display:flex><span>              path: /home/kubernetes/bin/nvidia
</span></span><span style=display:flex><span>          - name: gib
</span></span><span style=display:flex><span>            hostPath:
</span></span><span style=display:flex><span>              path: /home/kubernetes/bin/gib
</span></span><span style=display:flex><span>          - name: shared-memory
</span></span><span style=display:flex><span>            emptyDir:
</span></span><span style=display:flex><span>              medium: <span style=color:#a31515>&#34;Memory&#34;</span>
</span></span><span style=display:flex><span>              sizeLimit: 250Gi
</span></span></code></pre></div><p>If in a future we want to create smaller workers that use a subset of GPUs in
the Node we should use also the <a href=./nvidia-dranet.md>NVIDIA GPU DRA Driver</a> to
ensure the allocated GPUs and NICs on the node are aligned for optimal
performance.</p><p>Validate the deployment is working checking the Pods status:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span> kubectl get pods -o wide
</span></span><span style=display:flex><span>NAME                                                           READY   STATUS      RESTARTS   AGE     IP              NODE                                          NOMINATED NODE   READINESS GATES
</span></span><span style=display:flex><span>a4-ray-cluster-gpu-group-worker-gzzt6                    1/1     Running     0          8m11s   10.48.4.6       gke-dranet-aojea-dranet-a4-54bd557d-1blr      &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>a4-ray-cluster-gpu-group-worker-hnsvx                    1/1     Running     0          8m11s   10.48.3.6       gke-dranet-aojea-dranet-a4-54bd557d-5w4l      &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>a4-ray-cluster-head                                      1/1     Running     0          8m11s   10.48.2.6       gke-dranet-aojea-default-pool-7abaddc3-n287   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><p>Check if <code>a4-ray-cluster-head-svc</code> Service has been created successfully:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get services a4-ray-cluster-head-svc
</span></span><span style=display:flex><span>NAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                AGE
</span></span><span style=display:flex><span>a4-ray-cluster-head-svc   ClusterIP   None         &lt;none&gt;        10001/TCP,8265/TCP,6379/TCP,8080/TCP   13m
</span></span></code></pre></div><p>Identify your RayCluster’s head pod:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ export HEAD_POD=<span style=color:#00f>$(</span>kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>$ echo $HEAD_POD
</span></span><span style=display:flex><span>a4-ray-cluster-head
</span></span></code></pre></div><p>Print the cluster resources:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl exec -it $HEAD_POD -- python -c <span style=color:#a31515>&#34;import pprint; import ray; ray.init(); pprint.pprint(ray.cluster_resources(), sort_dicts=True)&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2025-07-14 10:44:41,326 INFO worker.py:1520 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS
</span></span><span style=display:flex><span>2025-07-14 10:44:41,327 INFO worker.py:1660 -- Connecting to existing Ray cluster at address: 10.48.2.6:6379...
</span></span><span style=display:flex><span>2025-07-14 10:44:41,343 INFO worker.py:1843 -- Connected to Ray cluster. View the dashboard at 10.48.2.6:8265
</span></span><span style=display:flex><span>{<span style=color:#a31515>&#39;CPU&#39;</span>: 402.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;GPU&#39;</span>: 16.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;accelerator_type:B200&#39;</span>: 2.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;memory&#39;</span>: 3438653071770.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;node:10.48.2.6&#39;</span>: 1.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;node:10.48.3.6&#39;</span>: 1.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;node:10.48.4.6&#39;</span>: 1.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;node:__internal_head__&#39;</span>: 1.0,
</span></span><span style=display:flex><span> <span style=color:#a31515>&#39;object_store_memory&#39;</span>: 401148243558.0}
</span></span></code></pre></div><p>Forward the port and check Ray dashboard:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl port-forward svc/a4-ray-cluster-head-svc 8265:8265
</span></span><span style=display:flex><span>Forwarding from 127.0.0.1:8265 -&gt; 8265
</span></span><span style=display:flex><span>Forwarding from [::1]:8265 -&gt; 8265
</span></span><span style=display:flex><span>Handling connection <span style=color:#00f>for</span> 8265
</span></span></code></pre></div><h4 id=gpu-to-gpu-using-ray-collective-communication-library>GPU-to-GPU using Ray Collective Communication Library</h4><p>Create a python file with the following code named <code>nccl_allreduce_multigpu.py</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00f>import</span> ray
</span></span><span style=display:flex><span><span style=color:#00f>import</span> torch
</span></span><span style=display:flex><span><span style=color:#00f>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>import</span> ray.util.collective <span style=color:#00f>as</span> collective
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@ray.remote(num_gpus=8)
</span></span><span style=display:flex><span><span style=color:#00f>class</span> <span style=color:#2b91af>Worker</span>:
</span></span><span style=display:flex><span>    <span style=color:#00f>def</span> __init__(self):
</span></span><span style=display:flex><span>        self.send_tensors = []
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:0&#39;</span>))
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:1&#39;</span>) * 2)
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:2&#39;</span>))
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:3&#39;</span>) * 2)
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:4&#39;</span>))
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:5&#39;</span>) * 2)
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:6&#39;</span>))
</span></span><span style=display:flex><span>        self.send_tensors.append(torch.ones((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:7&#39;</span>) * 2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self.recv = torch.zeros((4,), dtype=torch.float32, device=<span style=color:#a31515>&#39;cuda:0&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>def</span> setup(self, world_size, rank):
</span></span><span style=display:flex><span>        collective.init_collective_group(world_size, rank, <span style=color:#a31515>&#34;nccl&#34;</span>, <span style=color:#a31515>&#34;177&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> <span style=color:#00f>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>def</span> compute(self):
</span></span><span style=display:flex><span>        collective.allreduce_multigpu(self.send_tensors, <span style=color:#a31515>&#34;177&#34;</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        cpu_tensors = [t.cpu() <span style=color:#00f>for</span> t <span style=color:#00f>in</span> self.send_tensors]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> (
</span></span><span style=display:flex><span>            cpu_tensors,
</span></span><span style=display:flex><span>            self.send_tensors[0].device,
</span></span><span style=display:flex><span>            self.send_tensors[1].device,
</span></span><span style=display:flex><span>            self.send_tensors[2].device,
</span></span><span style=display:flex><span>            self.send_tensors[3].device,
</span></span><span style=display:flex><span>            self.send_tensors[4].device,
</span></span><span style=display:flex><span>            self.send_tensors[5].device,
</span></span><span style=display:flex><span>            self.send_tensors[6].device,
</span></span><span style=display:flex><span>            self.send_tensors[7].device,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>def</span> destroy(self):
</span></span><span style=display:flex><span>        collective.destroy_collective_group(<span style=color:#a31515>&#34;177&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> __name__ == <span style=color:#a31515>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    ray.init(address=<span style=color:#a31515>&#34;auto&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    num_workers = 2 
</span></span><span style=display:flex><span>    workers = []
</span></span><span style=display:flex><span>    init_rets = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>for</span> i <span style=color:#00f>in</span> range(num_workers):
</span></span><span style=display:flex><span>        w = Worker.remote()
</span></span><span style=display:flex><span>        workers.append(w)
</span></span><span style=display:flex><span>        init_rets.append(w.setup.remote(num_workers, i))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ray.get(init_rets)
</span></span><span style=display:flex><span>    print(<span style=color:#a31515>&#34;Collective groups initialized.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results = ray.get([w.compute.remote() <span style=color:#00f>for</span> w <span style=color:#00f>in</span> workers])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    print(<span style=color:#a31515>&#34;</span><span style=color:#a31515>\n</span><span style=color:#a31515>--- Allreduce Results ---&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#00f>for</span> i, (tensors_list, *devices) <span style=color:#00f>in</span> enumerate(results):
</span></span><span style=display:flex><span>        print(<span style=color:#a31515>f</span><span style=color:#a31515>&#34;Worker </span><span style=color:#a31515>{</span>i<span style=color:#a31515>}</span><span style=color:#a31515> results:&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#00f>for</span> j, tensor <span style=color:#00f>in</span> enumerate(tensors_list):
</span></span><span style=display:flex><span>            print(<span style=color:#a31515>f</span><span style=color:#a31515>&#34;  Tensor </span><span style=color:#a31515>{</span>j<span style=color:#a31515>}</span><span style=color:#a31515> (originally on </span><span style=color:#a31515>{</span>devices[j]<span style=color:#a31515>}</span><span style=color:#a31515>): </span><span style=color:#a31515>{</span>tensor<span style=color:#a31515>}</span><span style=color:#a31515>&#34;</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ray.get([w.destroy.remote() <span style=color:#00f>for</span> w <span style=color:#00f>in</span> workers])
</span></span><span style=display:flex><span>    print(<span style=color:#a31515>&#34;</span><span style=color:#a31515>\n</span><span style=color:#a31515>Collective groups destroyed.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ray.shutdown()
</span></span></code></pre></div><p>Create Ray job (should be created with the previously port forwarded, in this
case 8265):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ ray job submit --address=<span style=color:#a31515>&#34;http://localhost:8265&#34;</span> --runtime-env-json=<span style=color:#a31515>&#39;{&#34;working_dir&#34;: &#34;.&#34;, &#34;pip&#34;: [&#34;torch&#34;]}&#39;</span> -- python nccl_allreduce_multigpu.py
</span></span><span style=display:flex><span>Job submission server address: http://localhost:8265
</span></span><span style=display:flex><span>2025-07-14 17:32:08,731 INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_ec361f13f7b82502.zip.
</span></span><span style=display:flex><span>2025-07-14 17:32:08,733 INFO packaging.py:588 -- Creating a file package <span style=color:#00f>for</span> local module <span style=color:#a31515>&#39;.&#39;</span>.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>-------------------------------------------------------
</span></span><span style=display:flex><span>Job <span style=color:#a31515>&#39;raysubmit_QQTKZQDTDA3ifPMW&#39;</span> submitted successfully
</span></span><span style=display:flex><span>-------------------------------------------------------
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Next steps
</span></span><span style=display:flex><span>  Query the logs of the job:
</span></span><span style=display:flex><span>    ray job logs raysubmit_QQTKZQDTDA3ifPMW
</span></span><span style=display:flex><span>  Query the status of the job:
</span></span><span style=display:flex><span>    ray job status raysubmit_QQTKZQDTDA3ifPMW
</span></span><span style=display:flex><span>  Request the job to be stopped:
</span></span><span style=display:flex><span>    ray job stop raysubmit_QQTKZQDTDA3ifPMW
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Tailing logs <span style=color:#00f>until</span> the job exits (disable with --no-wait):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;snipped&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>--- Allreduce Results ---
</span></span><span style=display:flex><span>Worker 0 results:
</span></span><span style=display:flex><span>(Worker pid=3590, ip=10.48.4.17) id=0x15b3, options=0x0, comp_mask=0x0}
</span></span><span style=display:flex><span>(Worker pid=3590, ip=10.48.4.17) a4-ray-cluster-gpu-group-worker-pbkpw:3590:3778 [6] NCCL INFO NET/gIB: IbDev 6 Port 1 qpn 2440 se
</span></span><span style=display:flex><span>  Tensor 0 (originally on cuda:0): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 1 (originally on cuda:1): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 2 (originally on cuda:2): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 3 (originally on cuda:3): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 4 (originally on cuda:4): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 5 (originally on cuda:5): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 6 (originally on cuda:6): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 7 (originally on cuda:7): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>Worker 1 results:
</span></span><span style=display:flex><span>  Tensor 0 (originally on cuda:0): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 1 (originally on cuda:1): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 2 (originally on cuda:2): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 3 (originally on cuda:3): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 4 (originally on cuda:4): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 5 (originally on cuda:5): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 6 (originally on cuda:6): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>  Tensor 7 (originally on cuda:7): tensor([24., 24., 24., 24.])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;snipped&gt;
</span></span></code></pre></div><p>Since we are setting the informational NCCL environment variables NCCL_DEBUG and
NCCL_DEBUG_SUBSYS we can verify in the logs that RDMA GPUDirect is being used:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># [... snipped ...]</span>
</span></span><span style=display:flex><span><span style=color:green># The gIB (InfiniBand) plugin is initialized</span>
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3753 [2] NCCL INFO NET/gIB : Initializing gIB v1.0.6 [cite: 1887]
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3753 [2] NCCL INFO Initialized NET plugin gIB [cite: 1889]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Environment variable for GPU Direct RDMA level is detected</span>
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3754 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PIX [cite: 59]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># NCCL confirms that GPU Direct RDMA is enabled for each HCA (NIC) and GPU pairing</span>
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3758 [7] NCCL INFO NET/gIB : GPU Direct RDMA Enabled <span style=color:#00f>for</span> HCA 0 <span style=color:#a31515>&#39;mlx5_0&#39;</span> [cite: 41]
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3754 [3] NCCL INFO GPU Direct RDMA Enabled <span style=color:#00f>for</span> GPU 7 / HCA 0 (distance 4 &lt;= 4), read 0 mode Default [cite: 66]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Finally, communication channels are established using GDRDMA</span>
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3799 [2] NCCL INFO Channel 02/0 : 10[2] -&gt; 2[2] [receive] via NET/gIB/2/GDRDMA [cite: 1734]
</span></span><span style=display:flex><span>[cite_start][... (Worker pid=3590, ip=10.48.4.17) [0m a4-ray-cluster-gpu-group-worker-pbkpw:3590:3799 [2] NCCL INFO Channel 02/0 : 2[2] -&gt; 10[2] [send] via NET/gIB/2/GDRDMA [cite: 1739]
</span></span><span style=display:flex><span><span style=color:green># [... snipped ...]</span>
</span></span></code></pre></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="SIG Network mailing list" aria-label="SIG Network mailing list"><a target=_blank rel=noopener href=https://groups.google.com/forum/#!forum/kubernetes-sig-network aria-label="SIG Network mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/google/dranet aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://kubernetes.slack.com/messages/sig-network aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024&ndash;2025
<span class=td-footer__authors>Google LLC | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.d9615597e83c4193e3ab1e6816bad5c8741894e92c5b5c17a8d1d4be6d9af8a2.js integrity="sha256-2WFVl+g8QZPjqx5oFrrVyHQYlOksW1wXqNHUvm2a+KI=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>