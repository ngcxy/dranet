<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>User Guides on DraNet</title><link>https://dranet.dev/docs/user/</link><description>Recent content in User Guides on DraNet</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 14 Jul 2025 10:10:40 +0000</lastBuildDate><atom:link href="https://dranet.dev/docs/user/index.xml" rel="self" type="application/rss+xml"/><item><title>Ray on GKE using DraNet</title><link>https://dranet.dev/docs/user/kuberay/</link><pubDate>Mon, 14 Jul 2025 10:10:40 +0000</pubDate><guid>https://dranet.dev/docs/user/kuberay/</guid><description>To get started, follow the instructions to create a GKE cluster with DRA support and using DraNet, it is important to follow the instructions, since there are multiple dependencies on the Kubernetes API version, the RDMA NCCL installer and the DraNet component.
The worker nodes in this configuration are a4-highgpu-8g instances, each equipped with eight NVIDIA B200 GPUs and eight RDMA-capable RoCE NICs.
Deploy RayCluster Install Ray CRDs and the KubeRay operator:</description></item><item><title>GKE with NVIDIA DRA and DraNet</title><link>https://dranet.dev/docs/user/nvidia-dranet/</link><pubDate>Fri, 20 Jun 2025 10:10:40 +0000</pubDate><guid>https://dranet.dev/docs/user/nvidia-dranet/</guid><description>To get started, create a GKE cluster with DRA support and the corresponding VPC and subnets
It should look like
PROJECT=&amp;#34;gke-dranet&amp;#34; CLUSTER=&amp;#34;dranet-dranet&amp;#34; REGION=&amp;#34;us-west8&amp;#34; ZONE=&amp;#34;us-west8-c&amp;#34; GVNIC_NETWORK_PREFIX=&amp;#34;dranet-gvnic&amp;#34; RDMA_NETWORK_PREFIX=&amp;#34;dranet-rdma&amp;#34; VERSION=&amp;#34;1.33&amp;#34; gcloud container clusters create &amp;#34;${CLUSTER}&amp;#34; \ --cluster-version=&amp;#34;${VERSION}&amp;#34; \ --enable-multi-networking \ --enable-dataplane-v2 \ --enable-kubernetes-unstable-apis=resource.k8s.io/v1beta1/deviceclasses,resource.k8s.io/v1beta1/resourceclaims,resource.k8s.io/v1beta1/resourceclaimtemplates,resource.k8s.io/v1beta1/resourceslices \ --no-enable-autorepair \ --no-enable-autoupgrade \ --zone=&amp;#34;${ZONE}&amp;#34; \ --project=&amp;#34;${PROJECT}&amp;#34; # Create a VPC for the additional Google Titanium CPU NIC gcloud compute --project=${PROJECT?} \ networks create \ ${GVNIC_NETWORK_PREFIX?}-net \ --subnet-mode=custom gcloud compute --project=${PROJECT?</description></item><item><title>GKE and Cloud TPU v6e (Trillium)</title><link>https://dranet.dev/docs/user/gke-tpu-performance/</link><pubDate>Tue, 27 May 2025 11:30:40 +0000</pubDate><guid>https://dranet.dev/docs/user/gke-tpu-performance/</guid><description>If you use TPU Trillium and you want to improve the network performance of your Pods you can balance your network traffic over the VM NICs.
The ct6e-standard-4t machine type is backed by two physical NICs, since the main interface of the VM is used for all the applications and Pods on the host, you can create two additional vNICs on the VM that will be attached to each of the physical NICs, and pass them to the Pod directly, so you can multiplex your traffic to consume the total capacity of the physical NICs.</description></item><item><title>GKE and GPUDirect RDMA with DRA</title><link>https://dranet.dev/docs/user/gke-rdma/</link><pubDate>Tue, 27 May 2025 11:30:40 +0000</pubDate><guid>https://dranet.dev/docs/user/gke-rdma/</guid><description>On Google Cloud A3 Ultra and A4 machine types, you can utilize GPUDirect RDMA to run distributed AI workloads that require high performance networking support. To get started, create a GKE cluster with DRA support and the corresponding VPC and subnets for the RDMA network for the A3Ultra or A4 Node Pools, the gcloud commands should be something like:
PROJECT=&amp;#34;gke-dranet&amp;#34; CLUSTER=&amp;#34;dranet-dranet&amp;#34; REGION=&amp;#34;us-west8&amp;#34; ZONE=&amp;#34;us-west8-c&amp;#34; GVNIC_NETWORK_PREFIX=&amp;#34;dranet-gvnic&amp;#34; RDMA_NETWORK_PREFIX=&amp;#34;dranet-rdma&amp;#34; VERSION=&amp;#34;1.33&amp;#34; gcloud container clusters create &amp;#34;${CLUSTER}&amp;#34; \ --cluster-version=&amp;#34;${VERSION}&amp;#34; \ --enable-multi-networking \ --enable-dataplane-v2 \ --enable-kubernetes-unstable-apis=resource.</description></item><item><title>MPI Operator on GKE and GPUDirect RDMA</title><link>https://dranet.dev/docs/user/mpi-operator/</link><pubDate>Tue, 27 May 2025 11:30:40 +0000</pubDate><guid>https://dranet.dev/docs/user/mpi-operator/</guid><description>Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA&amp;rsquo;s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.
The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs â€“ for instance, one GPU and one dedicated RDMA-capable NIC per worker.</description></item><item><title>Interface Configuration</title><link>https://dranet.dev/docs/user/interface-configuration/</link><pubDate>Sun, 25 May 2025 11:30:40 +0000</pubDate><guid>https://dranet.dev/docs/user/interface-configuration/</guid><description>To configure network interfaces in DraNet, users can provide custom configurations through the parameters field of a ResourceClaim or ResourceClaimTemplate. This configuration adheres to the NetworkConfig structure, which defines the desired state for network interfaces and their associated routes.
Network Configuration Overview The primary structure for custom network configuration is NetworkConfig. It encompasses settings for the network interface itself and any specific routes to be applied within the Pod&amp;rsquo;s network namespace.</description></item></channel></rss>