<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>MPI Operator on GKE | DraNet</title>
<meta name=description content="Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA’s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.
The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs – for instance, one GPU and one dedicated RDMA-capable NIC per worker."><meta property="og:url" content="https://dranet.dev/docs/user/mpi-operator/"><meta property="og:site_name" content="DraNet"><meta property="og:title" content="MPI Operator on GKE"><meta property="og:description" content="Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA’s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.
The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs – for instance, one GPU and one dedicated RDMA-capable NIC per worker."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-05-27T11:30:40+00:00"><meta property="article:modified_time" content="2025-05-27T11:30:40+00:00"><meta itemprop=name content="MPI Operator on GKE"><meta itemprop=description content="Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA&rsquo;s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.
The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs – for instance, one GPU and one dedicated RDMA-capable NIC per worker."><meta itemprop=datePublished content="2025-05-27T11:30:40+00:00"><meta itemprop=dateModified content="2025-05-27T11:30:40+00:00"><meta itemprop=wordCount content="1312"><meta name=twitter:card content="summary"><meta name=twitter:title content="MPI Operator on GKE"><meta name=twitter:description content="Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA&rsquo;s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.
The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs – for instance, one GPU and one dedicated RDMA-capable NIC per worker."><link rel=preload href=/scss/main.min.48c25d0a5a23a1e8cae94d6c5e7622061e5345cf098171b1d6ee41d8e309e6c8.css as=style integrity="sha256-SMJdClojoejK6U1sXnYiBh5TRc8JgXGx1u5B2OMJ5sg=" crossorigin=anonymous><link href=/scss/main.min.48c25d0a5a23a1e8cae94d6c5e7622061e5345cf098171b1d6ee41d8e309e6c8.css rel=stylesheet integrity="sha256-SMJdClojoejK6U1sXnYiBh5TRc8JgXGx1u5B2OMJ5sg=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YH3W884R6Z"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YH3W884R6Z")}</script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>DraNet</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/docs/user><span>User Guides</span></a></li><li class=nav-item><a class=nav-link href=/docs/concepts><span>Concepts</span></a></li><li class=nav-item><a class=nav-link href=/docs/contributing><span>Contributing</span></a></li></ul></div><div class="d-none d-lg-block"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="td-sidebar-nav collapse" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docs-li><a href=/docs/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-docs><span>DraNet</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsquick-start-li><a href=/docs/quick-start/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsquick-start><span>Quick Start</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docsuser-li><a href=/docs/user/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsuser><span>User Guides</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-docsusermpi-operator-li><a href=/docs/user/mpi-operator/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id=m-docsusermpi-operator><span class=td-sidebar-nav-active-item>MPI Operator on GKE</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsuserinterface-configuration-li><a href=/docs/user/interface-configuration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsuserinterface-configuration><span>Interface Configuration</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsconcepts-li><a href=/docs/concepts/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsconcepts><span>Concepts</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsinterface-status-li><a href=/docs/concepts/interface-status/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsinterface-status><span>Interface Status</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptshardware-efficiency-li><a href=/docs/concepts/hardware-efficiency/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptshardware-efficiency><span>Hardware Efficiency</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsrdma-li><a href=/docs/concepts/rdma/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsrdma><span>RDMA</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsrdma-modes-li><a href=/docs/concepts/rdma-modes/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsrdma-modes><span>RDMA Device Handling</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptshowitworks-li><a href=/docs/concepts/howitworks/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptshowitworks><span>How It Works</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsconceptsreferences-li><a href=/docs/concepts/references/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsconceptsreferences><span>References</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docscontributing-li><a href=/docs/contributing/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docscontributing><span>Contributing</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docscontributingdeveloper-guide-li><a href=/docs/contributing/developer-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributingdeveloper-guide><span>Developer Guide</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docscontributingcontributing-li><a href=/docs/contributing/contributing/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docscontributingcontributing><span>Contributing</span></a></li></ul></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"><a href=https://github.com/google/dranet/tree/main/site/content/docs/user/mpi-operator.md class="td-page-meta--view td-page-meta__view" target=_blank rel=noopener><i class="fa-solid fa-file-lines fa-fw"></i> View page source</a>
<a href=https://github.com/google/dranet/edit/main/site/content/docs/user/mpi-operator.md class="td-page-meta--edit td-page-meta__edit" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Edit this page</a>
<a href="https://github.com/google/dranet/new/main/site/content/docs/user?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class="td-page-meta--child td-page-meta__child" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Create child page</a>
<a href="https://github.com/google/dranet/issues/new?title=MPI%20Operator%20on%20GKE" class="td-page-meta--issue td-page-meta__issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create documentation issue</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#dranet--mpi-operator-a-powerful-combination>DraNet + MPI Operator: A Powerful Combination</a><ul><li><a href=#example-running-nccl-tests-for-distributed-workload-validation>Example: Running NCCL Tests for Distributed Workload Validation</a></li></ul></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/docs/>DraNet</a></li><li class=breadcrumb-item><a href=/docs/user/>User Guides</a></li><li class="breadcrumb-item active" aria-current=page>MPI Operator on GKE</li></ol></nav><div class=td-content><h1>MPI Operator on GKE</h1><header class=article-meta></header><p>Running distributed applications, such as those using the Message Passing Interface (MPI) or NVIDIA&rsquo;s Collective Communications Library (NCCL) for GPU communication, often requires each participating process (or Pod, in Kubernetes terms) to have access to high-speed, low-latency interconnects. Simply sharing a generic network interface among many high-performance jobs can lead to contention, unpredictable performance, and underutilization of expensive hardware.</p><p>The goal is resource compartmentalization: ensuring that each part of your distributed job gets dedicated access to the specific resources it needs – for instance, one GPU and one dedicated RDMA-capable NIC per worker.</p><h2 id=dranet--mpi-operator-a-powerful-combination>DraNet + MPI Operator: A Powerful Combination</h2><ul><li><p>DraNet: Provides the mechanism to discover RDMA-capable NICs on your Kubernetes nodes and make them available for Pods to claim. Through DRA, Pods can request a specific NIC, and DraNet, via NRI hooks, will configure it within the Pod&rsquo;s namespace, <a href=google/dranet/dranet-dcd98f563b1a24f4800cf3d2d502ec5b2f488ddc/site/content/docs/user/interface-configuration.md>even naming it predictably (e.g., dranet0)</a></p></li><li><p><a href=https://github.com/kubeflow/mpi-operator>Kubeflow MPI Operator</a>: Simplifies the deployment and management of MPI-based applications on Kubernetes. It handles the setup of MPI ranks, hostfiles, and the execution of mpirun.</p></li></ul><p>By using them together, we can create MPIJob definitions where each worker Pod explicitly claims a dedicated RDMA NIC managed by DraNet, alongside its GPU</p><h3 id=example-running-nccl-tests-for-distributed-workload-validation>Example: Running NCCL Tests for Distributed Workload Validation</h3><p>A common and reliable way to validate that that our distributed setup is performing optimally is by running an <a href=https://github.com/NVIDIA/nccl-tests>NVIDIA&rsquo;s Collective Communications Library (NCCL) All-Reduce test</a>. This benchmark is designed to exercise the high-speed interconnects between nodes, helping you confirm that the RDMA fabric (like InfiniBand or RoCE) is operating correctly and ready to support your distributed workloads with expected efficiency.</p><p>Let&rsquo;s see how we can run this with DraNet and the MPI Operator, focusing on a 1 GPU and 1 NIC per worker configuration.</p><h4 id=defining-resources-for-dranet>Defining Resources for DraNet</h4><p>First, we tell DraNet what kind of NICs we&rsquo;re interested in and how Pods can claim them.</p><p><strong>DeviceClass (dranet-rdma-for-mpi):</strong> This selects RDMA-capable NICs managed by DraNet.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: resource.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: DeviceClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dranet-rdma-for-mpi
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selectors:
</span></span><span style=display:flex><span>    - cel:
</span></span><span style=display:flex><span>        expression: device.driver == &#34;dra.net&#34;
</span></span><span style=display:flex><span>    - cel:
</span></span><span style=display:flex><span>        expression: device.attributes[&#34;dra.net&#34;].rdma == true
</span></span></code></pre></div><p><strong>ResourceClaimTemplate (mpi-worker-rdma-nic-template):</strong> MPI worker Pods will use this to request one RDMA NIC. DraNet will be instructed to name this interface dranet0 inside the Pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: resource.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ResourceClaimTemplate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: mpi-worker-rdma-nic-template
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    devices:
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        - name: rdma-nic-for-mpi
</span></span><span style=display:flex><span>          deviceClassName: dranet-rdma-for-mpi
</span></span><span style=display:flex><span>          selectors:
</span></span><span style=display:flex><span>          - cel:
</span></span><span style=display:flex><span>              expression: device.attributes[&#34;dra.net&#34;].ifName == &#34;gpu2rdma0&#34;
</span></span><span style=display:flex><span>    config:
</span></span><span style=display:flex><span>    - opaque:
</span></span><span style=display:flex><span>        driver: dra.net
</span></span><span style=display:flex><span>        parameters:
</span></span><span style=display:flex><span>          interface:
</span></span><span style=display:flex><span>            name: <span style=color:#a31515>&#34;dranet0&#34;</span> <span style=color:green># NCCL will use this interface</span>
</span></span></code></pre></div><h4 id=install-the-gke-optimized-rdma-dependencies>Install the GKE optimized RDMA dependencies</h4><p>GKE automatically install on the VM some optimized RDMA and NCCL libraries for Google Cloud infrastructure, that can be installed following the instructions on:</p><p><a href=https://cloud.google.com/ai-hypercomputer/docs/create/gke-ai-hypercompute-custom#install-rdma-configure-nccl>https://cloud.google.com/ai-hypercomputer/docs/create/gke-ai-hypercompute-custom#install-rdma-configure-nccl</a></p><p>In order to use them you need to mount the following volumes</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>    - name: library-dir-host
</span></span><span style=display:flex><span>      hostPath:
</span></span><span style=display:flex><span>        path: /home/kubernetes/bin/nvidia
</span></span><span style=display:flex><span>    - name: gib
</span></span><span style=display:flex><span>      hostPath:
</span></span><span style=display:flex><span>        path: /home/kubernetes/bin/gib
</span></span></code></pre></div><p>in your workloads:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>containers:
</span></span><span style=display:flex><span>  - name: my-container
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>      - name: library-dir-host
</span></span><span style=display:flex><span>        mountPath: /usr/local/nvidia
</span></span><span style=display:flex><span>      - name: gib
</span></span><span style=display:flex><span>        mountPath: /usr/local/gib
</span></span><span style=display:flex><span>    env:
</span></span><span style=display:flex><span>      - name: LD_LIBRARY_PATH
</span></span><span style=display:flex><span>        value: /usr/local/nvidia/lib64
</span></span></code></pre></div><h4 id=crafting-the-mpijob>Crafting the MPIJob</h4><p>The MPIJob specification is where we tie everything together. We&rsquo;ll define a job with two workers, each getting one GPU and one DraNet-managed RDMA NIC.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: kubeflow.org/v2beta1
</span></span><span style=display:flex><span>kind: MPIJob
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nccl-test-dranet-1gpu-1nic
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  slotsPerWorker: 1 <span style=color:green># 1 MPI rank per worker Pod</span>
</span></span><span style=display:flex><span>  mpiReplicaSpecs:
</span></span><span style=display:flex><span>    Launcher:
</span></span><span style=display:flex><span>      replicas: 1
</span></span><span style=display:flex><span>      template:
</span></span><span style=display:flex><span>        spec:
</span></span><span style=display:flex><span>          containers:
</span></span><span style=display:flex><span>          - image: mpioperator/openmpi:v0.6.0
</span></span><span style=display:flex><span>            name: mpi-launcher
</span></span><span style=display:flex><span>            command: [<span style=color:#a31515>&#34;/bin/bash&#34;</span>, <span style=color:#a31515>&#34;-c&#34;</span>]
</span></span><span style=display:flex><span>            args:
</span></span><span style=display:flex><span>            - |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>              set -ex
</span></span></span><span style=display:flex><span><span style=color:#a31515>              mpirun \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                --allow-run-as-root \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                --prefix /opt/openmpi \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                -np 2 \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                -bind-to none \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                -map-by slot \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                -mca routed direct \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                -x LD_LIBRARY_PATH=/usr/local/nvidia/lib64 \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                bash -c \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                  &#34;source /usr/local/gib/scripts/set_nccl_env.sh; \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                  /usr/local/bin/all_reduce_perf \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                    -g 1 -b 1K -e 8G -f 2 \
</span></span></span><span style=display:flex><span><span style=color:#a31515>                    -w 5 -n 20;&#34;</span>              
</span></span><span style=display:flex><span>            securityContext:
</span></span><span style=display:flex><span>              capabilities:
</span></span><span style=display:flex><span>                add: [<span style=color:#a31515>&#34;IPC_LOCK&#34;</span>]
</span></span><span style=display:flex><span>    Worker:
</span></span><span style=display:flex><span>      replicas: 2
</span></span><span style=display:flex><span>      template:
</span></span><span style=display:flex><span>        spec:
</span></span><span style=display:flex><span>          resourceClaims:
</span></span><span style=display:flex><span>          - name: worker-rdma-nic
</span></span><span style=display:flex><span>            resourceClaimTemplateName: mpi-worker-rdma-nic-template
</span></span><span style=display:flex><span>          containers:
</span></span><span style=display:flex><span>          - image: ghcr.io/google/dranet-rdma-perftest:sha-fb3f932
</span></span><span style=display:flex><span>            name: mpi-worker
</span></span><span style=display:flex><span>            securityContext:
</span></span><span style=display:flex><span>              capabilities:
</span></span><span style=display:flex><span>                add: [<span style=color:#a31515>&#34;IPC_LOCK&#34;</span>]
</span></span><span style=display:flex><span>            resources:
</span></span><span style=display:flex><span>              limits:
</span></span><span style=display:flex><span>                nvidia.com/gpu: 1 <span style=color:green># Each worker gets 1 GPU</span>
</span></span><span style=display:flex><span>            volumeMounts:
</span></span><span style=display:flex><span>              - name: library-dir-host
</span></span><span style=display:flex><span>                mountPath: /usr/local/nvidia
</span></span><span style=display:flex><span>              - name: gib
</span></span><span style=display:flex><span>                mountPath: /usr/local/gib
</span></span><span style=display:flex><span>          volumes:
</span></span><span style=display:flex><span>            - name: library-dir-host
</span></span><span style=display:flex><span>              hostPath:
</span></span><span style=display:flex><span>                path: /home/kubernetes/bin/nvidia
</span></span><span style=display:flex><span>            - name: gib
</span></span><span style=display:flex><span>              hostPath:
</span></span><span style=display:flex><span>                path: /home/kubernetes/bin/gib
</span></span></code></pre></div><h4 id=running-and-observing>Running and Observing</h4><p>Once deployed, the MPI Operator will launch the job. The launcher Pod will execute mpirun, which starts the all_reduce_perf test across the two worker Pods. Each worker Pod will use its dedicated GPU and its dedicated dranet0 (RDMA NIC) for NCCL communications.</p><p>You can monitor the launcher&rsquo;s logs to see the NCCL benchmark results, including the achieved bus bandwidth.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl logs <span style=color:#00f>$(</span>kubectl get pods | grep launcher | awk <span style=color:#a31515>&#39;{ print $1}&#39;</span><span style=color:#00f>)</span> -f
</span></span><span style=display:flex><span>+ mpirun --allow-run-as-root --prefix /opt/openmpi -np 2 -bind-to none -map-by slot -mca routed direct -x LD_LIBRARY_PATH=/usr/local/nvidia/lib64 bash -c <span style=color:#a31515>&#39;source /usr/local/gib/scripts/set_nccl_env.sh;     /usr/local/bin/all_reduce_perf       -g 1 -b 1K -e 8G -f 2       -w 5 -n 20;&#39;</span>
</span></span><span style=display:flex><span>Warning: Permanently added <span style=color:#a31515>&#39;[nccl-test-dranet-1gpu-1nic-worker-1.nccl-test-dranet-1gpu-1nic.default.svc]:2222&#39;</span> (ED25519) to the list of known hosts.
</span></span><span style=display:flex><span>Warning: Permanently added <span style=color:#a31515>&#39;[nccl-test-dranet-1gpu-1nic-worker-0.nccl-test-dranet-1gpu-1nic.default.svc]:2222&#39;</span> (ED25519) to the list of known hosts.
</span></span><span style=display:flex><span>--------------------------------------------------------------------------
</span></span><span style=display:flex><span>WARNING: No preset parameters were found <span style=color:#00f>for</span> the device that Open MPI
</span></span><span style=display:flex><span>detected:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  Local host:            nccl-test-dranet-1gpu-1nic-worker-0
</span></span><span style=display:flex><span>  Device name:           mlx5_2
</span></span><span style=display:flex><span>  Device vendor ID:      0x02c9
</span></span><span style=display:flex><span>  Device vendor part ID: 4126
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Default device parameters will be used, which may result in lower
</span></span><span style=display:flex><span>performance.  You can edit any of the files specified by the
</span></span><span style=display:flex><span>btl_openib_device_param_files MCA parameter to set values <span style=color:#00f>for</span> your
</span></span><span style=display:flex><span>device.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NOTE: You can turn off this warning by setting the MCA parameter
</span></span><span style=display:flex><span>      btl_openib_warn_no_device_params_found to 0.
</span></span><span style=display:flex><span>--------------------------------------------------------------------------
</span></span><span style=display:flex><span>--------------------------------------------------------------------------
</span></span><span style=display:flex><span>No OpenFabrics connection schemes reported that they were able to be
</span></span><span style=display:flex><span>used on a specific port.  As such, the openib BTL (OpenFabrics
</span></span><span style=display:flex><span>support) will be disabled <span style=color:#00f>for</span> this port.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  Local host:           nccl-test-dranet-1gpu-1nic-worker-0
</span></span><span style=display:flex><span>  Local device:         mlx5_2
</span></span><span style=display:flex><span>  Local port:           1
</span></span><span style=display:flex><span>  CPCs attempted:       rdmacm, udcm
</span></span><span style=display:flex><span>--------------------------------------------------------------------------
</span></span><span style=display:flex><span><span style=color:green># nThread 1 nGpus 1 minBytes 1024 maxBytes 8589934592 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># Using devices</span>
</span></span><span style=display:flex><span><span style=color:green>#  Rank  0 Group  0 Pid     23 on nccl-test-dranet-1gpu-1nic-worker-0 device  0 [0000:cc:00] NVIDIA H200</span>
</span></span><span style=display:flex><span><span style=color:green>#  Rank  1 Group  0 Pid     21 on nccl-test-dranet-1gpu-1nic-worker-1 device  0 [0000:97:00] NVIDIA H200</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green>#                                                              out-of-place                       in-place</span>
</span></span><span style=display:flex><span><span style=color:green>#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span>
</span></span><span style=display:flex><span><span style=color:green>#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)</span>
</span></span><span style=display:flex><span>        1024           256     float     sum      -1    35.59    0.03    0.03      0    30.61    0.03    0.03      0
</span></span><span style=display:flex><span>        2048           512     float     sum      -1    31.80    0.06    0.06      0    31.90    0.06    0.06      0
</span></span><span style=display:flex><span>        4096          1024     float     sum      -1    33.56    0.12    0.12      0    33.33    0.12    0.12      0
</span></span><span style=display:flex><span>        8192          2048     float     sum      -1    39.33    0.21    0.21      0    39.24    0.21    0.21      0
</span></span><span style=display:flex><span>       16384          4096     float     sum      -1    41.89    0.39    0.39      0    40.31    0.41    0.41      0
</span></span><span style=display:flex><span>       32768          8192     float     sum      -1    45.47    0.72    0.72      0    42.92    0.76    0.76      0
</span></span><span style=display:flex><span>       65536         16384     float     sum      -1    54.03    1.21    1.21      0    51.81    1.26    1.26      0
</span></span><span style=display:flex><span>      131072         32768     float     sum      -1    51.86    2.53    2.53      0    52.60    2.49    2.49      0
</span></span><span style=display:flex><span>      262144         65536     float     sum      -1    79.10    3.31    3.31      0    68.36    3.83    3.83      0
</span></span><span style=display:flex><span>      524288        131072     float     sum      -1    76.88    6.82    6.82      0    76.38    6.86    6.86      0
</span></span><span style=display:flex><span>     1048576        262144     float     sum      -1    98.57   10.64   10.64      0    93.72   11.19   11.19      0
</span></span><span style=display:flex><span>     2097152        524288     float     sum      -1    131.9   15.90   15.90      0    131.8   15.91   15.91      0
</span></span><span style=display:flex><span>     4194304       1048576     float     sum      -1    227.5   18.44   18.44      0    227.4   18.45   18.45      0
</span></span><span style=display:flex><span>     8388608       2097152     float     sum      -1    415.7   20.18   20.18      0    416.7   20.13   20.13      0
</span></span><span style=display:flex><span>    16777216       4194304     float     sum      -1    811.3   20.68   20.68      0    808.5   20.75   20.75      0
</span></span><span style=display:flex><span>    33554432       8388608     float     sum      -1   1609.7   20.84   20.84      0   1607.6   20.87   20.87      0
</span></span><span style=display:flex><span>    67108864      16777216     float     sum      -1   2250.8   29.82   29.82      0   2253.3   29.78   29.78      0
</span></span><span style=display:flex><span>   134217728      33554432     float     sum      -1   4440.0   30.23   30.23      0   4444.3   30.20   30.20      0
</span></span><span style=display:flex><span>   268435456      67108864     float     sum      -1   8635.4   31.09   31.09      0   8653.9   31.02   31.02      0
</span></span><span style=display:flex><span>   536870912     134217728     float     sum      -1    17077   31.44   31.44      0    17081   31.43   31.43      0
</span></span><span style=display:flex><span>  1073741824     268435456     float     sum      -1    33860   31.71   31.71      0    33896   31.68   31.68      0
</span></span><span style=display:flex><span>  2147483648     536870912     float     sum      -1    67521   31.80   31.80      0    67503   31.81   31.81      0
</span></span><span style=display:flex><span>  4294967296    1073741824     float     sum      -1   134734   31.88   31.88      0   135069   31.80   31.80      0
</span></span><span style=display:flex><span>  8589934592    2147483648     float     sum      -1   269368   31.89   31.89      0   269407   31.88   31.88      0
</span></span><span style=display:flex><span><span style=color:green># Out of bounds values : 0 OK</span>
</span></span><span style=display:flex><span><span style=color:green># Avg bus bandwidth    : 15.5188</span>
</span></span></code></pre></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="SIG Network mailing list" aria-label="SIG Network mailing list"><a target=_blank rel=noopener href=https://groups.google.com/forum/#!forum/kubernetes-sig-network aria-label="SIG Network mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/google/dranet aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://kubernetes.slack.com/messages/sig-network aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024&ndash;2025
<span class=td-footer__authors>Google LLC | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.d9615597e83c4193e3ab1e6816bad5c8741894e92c5b5c17a8d1d4be6d9af8a2.js integrity="sha256-2WFVl+g8QZPjqx5oFrrVyHQYlOksW1wXqNHUvm2a+KI=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>